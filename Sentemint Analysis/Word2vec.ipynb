{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as r\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash32(value):\n",
    "     return hash(value) & 0xffffffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('TrainData.txt', 'r', encoding='utf-8')\n",
    "content = f.read()\n",
    "train = content.split(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('TrainDataLabel.txt', 'r', encoding='utf-8')\n",
    "content = f.read()\n",
    "trainLabel = content.split(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('TestData.txt', 'r', encoding='utf-8')\n",
    "content = f.read()\n",
    "test = content.split(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('TestDataLabel.txt', 'r', encoding='utf-8')\n",
    "content = f.read()\n",
    "testLabel = content.split(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def preprocess (sentence):  \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", sentence) \n",
    "    return(letters_only.split())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedTrain = []\n",
    "for s in train :\n",
    "    preprocessedTrain.append(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedTest= []\n",
    "for s in test :\n",
    "    preprocessedTest.append(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-17 22:17:40,759 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-04-17 22:17:40,761 : INFO : collecting all words and their counts\n",
      "2018-04-17 22:17:40,762 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-17 22:17:40,790 : INFO : collected 7799 word types from a corpus of 109674 raw words and 4971 sentences\n",
      "2018-04-17 22:17:40,791 : INFO : Loading a fresh vocabulary\n",
      "2018-04-17 22:17:40,800 : INFO : min_count=10 retains 1056 unique words (13% of original 7799, drops 6743)\n",
      "2018-04-17 22:17:40,805 : INFO : min_count=10 leaves 95354 word corpus (86% of original 109674, drops 14320)\n",
      "2018-04-17 22:17:40,815 : INFO : deleting the raw counts dictionary of 7799 items\n",
      "2018-04-17 22:17:40,816 : INFO : sample=0.001 downsamples 60 most-common words\n",
      "2018-04-17 22:17:40,817 : INFO : downsampling leaves estimated 60274 word corpus (63.2% of prior 95354)\n",
      "2018-04-17 22:17:40,821 : INFO : estimated required memory for 1056 words and 50 dimensions: 950400 bytes\n",
      "2018-04-17 22:17:40,822 : INFO : resetting layer weights\n",
      "2018-04-17 22:17:40,846 : INFO : training model with 4 workers on 1056 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-04-17 22:17:40,915 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-17 22:17:40,917 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-17 22:17:40,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-17 22:17:40,930 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-17 22:17:40,932 : INFO : EPOCH - 1 : training on 109674 raw words (60195 effective words) took 0.1s, 831134 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-17 22:17:41,006 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-17 22:17:41,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-17 22:17:41,017 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-17 22:17:41,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-17 22:17:41,022 : INFO : EPOCH - 2 : training on 109674 raw words (60215 effective words) took 0.1s, 762360 effective words/s\n",
      "2018-04-17 22:17:41,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-17 22:17:41,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-17 22:17:41,110 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-17 22:17:41,112 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-17 22:17:41,115 : INFO : EPOCH - 3 : training on 109674 raw words (60298 effective words) took 0.1s, 753272 effective words/s\n",
      "2018-04-17 22:17:41,187 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-17 22:17:41,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-17 22:17:41,198 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-17 22:17:41,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-17 22:17:41,203 : INFO : EPOCH - 4 : training on 109674 raw words (60230 effective words) took 0.1s, 832595 effective words/s\n",
      "2018-04-17 22:17:41,270 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-17 22:17:41,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-17 22:17:41,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-17 22:17:41,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-17 22:17:41,287 : INFO : EPOCH - 5 : training on 109674 raw words (60002 effective words) took 0.1s, 836830 effective words/s\n",
      "2018-04-17 22:17:41,291 : INFO : training on a 548370 raw words (300940 effective words) took 0.4s, 680477 effective words/s\n",
      "2018-04-17 22:17:41,296 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-04-17 22:17:41,315 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-04-17 22:17:41,317 : INFO : not storing attribute vectors_norm\n",
      "2018-04-17 22:17:41,318 : INFO : not storing attribute cum_table\n",
      "2018-04-17 22:17:41,337 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 50    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(preprocessedTrain, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('drunk', 0.9994156360626221),\n",
       " ('after', 0.9993057250976562),\n",
       " ('woman', 0.9991390109062195),\n",
       " ('accident', 0.9991375803947449),\n",
       " ('At', 0.9990721344947815),\n",
       " ('old', 0.9989862442016602),\n",
       " ('few', 0.9989776611328125),\n",
       " ('house', 0.9989644289016724),\n",
       " ('street', 0.9989177584648132),\n",
       " ('lost', 0.9989074468612671)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.14315182, -0.03457012,  0.05327838, ...,  0.18251769,\n",
       "         0.239316  ,  0.12071609],\n",
       "       [ 0.22973093, -0.03391451, -0.00988347, ...,  0.13167509,\n",
       "         0.22407228,  0.08504926],\n",
       "       [ 0.22385353, -0.02547994, -0.03545805, ...,  0.10333878,\n",
       "         0.22982496,  0.07525364],\n",
       "       ..., \n",
       "       [ 0.19956917, -0.02367841, -0.00199067, ...,  0.13782214,\n",
       "         0.22258505,  0.08451828],\n",
       "       [ 0.21096005, -0.00972852, -0.0092427 , ...,  0.13408367,\n",
       "         0.22952847,  0.08443928],\n",
       "       [ 0.16984349, -0.02145743, -0.02204535, ...,  0.17426214,\n",
       "         0.24434169,  0.06819768]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['I'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000 == 0:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 4971\n",
      "Review 1000 of 4971\n",
      "Review 2000 of 4971"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "E:\\Anaconda\\envs\\NoPytorch\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review 3000 of 4971\n",
      "Review 4000 of 4971\n",
      "Review 0 of 2311\n",
      "Review 1000 of 2311\n",
      "Review 2000 of 2311\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( preprocessedTrain, model, num_features )\n",
    "testDataVecs = getAvgFeatureVecs( preprocessedTest, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4971, 50)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2311, 50)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = np.nan_to_num(trainDataVecs)\n",
    "ts = np.nan_to_num(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print (\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( tv, trainLabel )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict( ts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.991778450887061"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testLabel, result) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
